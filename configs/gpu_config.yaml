# GPU Optimization Configuration for RTX 3090
# VR Body Segmentation Application

# Hardware Configuration
hardware:
  gpu:
    device_id: 0
    model: "NVIDIA RTX 3090"
    vram_gb: 24
    cuda_cores: 10496
    architecture: "Ampere"
  cpu:
    model: "AMD Ryzen Threadripper 3990X"
    cores: 64
    threads: 128
  system_ram_gb: 48

# CUDA Kernel Configuration
cuda:
  # Custom kernel settings
  kernels:
    enabled: true
    block_size: [16, 16]  # Optimal for Ampere architecture
    use_fused_kernels: true  # Combine operations for efficiency

  # Stream management
  streams:
    num_preprocess_streams: 4
    num_inference_streams: 2
    async_execution: true

# TensorRT Configuration
tensorrt:
  # Precision settings
  precision:
    fp16: true  # Enable FP16 for 2x speedup
    int8: false  # Enable INT8 for 4x speedup (requires calibration)
    fp32_fallback: true  # Fallback to FP32 for unsupported ops

  # Optimization settings
  optimization:
    workspace_size_gb: 8
    min_timing_iterations: 3
    avg_timing_iterations: 8
    max_batch_size: 16

  # Dynamic shapes (for variable batch sizes)
  dynamic_shapes:
    enabled: true
    batch_sizes:
      min: 1
      optimal: 4
      max: 8
    input_shapes:
      min: [1, 3, 512, 512]
      optimal: [4, 3, 512, 512]
      max: [8, 3, 512, 512]

  # Engine caching
  cache:
    enabled: true
    directory: "./tensorrt_cache"
    rebuild_on_input_change: true

# Memory Management
memory:
  # VRAM settings
  vram:
    max_utilization: 0.85  # Use up to 85% of VRAM
    alert_threshold: 0.90  # Alert when exceeding 90%
    emergency_threshold: 0.95  # Emergency cleanup at 95%

  # Memory pooling
  pooling:
    enabled: true
    pool_size_gb: 4.0
    chunk_sizes: [
      262144,      # 1 frame: 1*3*512*512
      1048576,     # 4 frames: 4*3*512*512
      2097152,     # 8 frames: 8*3*512*512
      16777216     # Feature maps: 1*64*512*512
    ]

  # Quantization settings
  quantization:
    model_precision: "fp16"  # fp32, fp16, or int8
    activation_precision: "fp16"
    weight_precision: "fp16"

  # Mixed precision training
  mixed_precision:
    enabled: true
    opt_level: "O2"  # O0, O1, O2, O3
    loss_scale: "dynamic"

# Batch Processing
batching:
  # Dynamic batching
  dynamic:
    enabled: true
    min_batch_size: 1
    max_batch_size: 8
    timeout_ms: 50  # Max wait time to accumulate batch

  # Adaptive batch sizing
  adaptive:
    enabled: true
    initial_batch_size: 4
    target_vram_utilization: 0.80
    adjustment_interval_frames: 100

  # Priority mode
  priority:
    enabled: false  # Prioritize low-latency over high-throughput

  # Batch optimization
  optimization:
    auto_find_optimal: true  # Automatically find optimal batch size
    num_warmup_iterations: 10
    num_test_iterations: 50

# Async Pipeline Configuration
pipeline:
  # Stage configuration
  stages:
    decode:
      threads: 8  # Parallel video decoding
      queue_size: 50

    preprocess:
      cuda_streams: 4
      queue_size: 50
      target_size: [512, 512]
      normalization:
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

    inference:
      batch_size: 4
      queue_size: 50
      warmup_iterations: 10

    postprocess:
      threads: 16  # Parallel CPU post-processing
      queue_size: 50

    encode:
      threads: 8
      queue_size: 50

  # Pipeline optimization
  optimization:
    overlap_cpu_gpu: true
    prefetch_batches: 2
    double_buffering: true

  # Backpressure handling
  backpressure:
    enabled: true
    max_queue_depth: 100
    drop_frames_on_overflow: false

# Multi-threading Configuration
threading:
  # CPU thread pool
  cpu:
    video_decode_threads: 8
    video_encode_threads: 8
    postprocess_threads: 16
    io_threads: 4

  # Thread affinity (pin threads to specific cores)
  affinity:
    enabled: false  # Enable for NUMA systems
    core_ranges:
      decode: [0, 15]
      postprocess: [16, 47]
      encode: [48, 63]

  # Thread priority
  priority:
    video_decode: "high"
    inference: "realtime"
    postprocess: "normal"

# Profiling Configuration
profiling:
  # Enable profiling
  enabled: true

  # What to profile
  targets:
    - "decode"
    - "preprocess"
    - "inference"
    - "postprocess"
    - "encode"
    - "memory"

  # Export settings
  export:
    enabled: true
    format: "json"  # json, chrome_trace, tensorboard
    directory: "./profiling_results"
    interval_seconds: 60  # Export every 60 seconds

  # Monitoring
  monitoring:
    log_interval_seconds: 5
    track_queue_depths: true
    track_memory: true
    detect_bottlenecks: true

# Performance Targets
targets:
  # Target performance metrics
  latency:
    max_end_to_end_ms: 50  # 50ms for VR responsiveness
    max_inference_ms: 20
    max_preprocess_ms: 5
    max_postprocess_ms: 10

  throughput:
    min_fps: 60  # 60 FPS for smooth VR
    target_fps: 90  # 90 FPS ideal for VR

  quality:
    min_iou: 0.85  # Minimum segmentation quality
    max_false_positive_rate: 0.05

# Optimization Strategies
strategies:
  # Latency vs Throughput trade-off
  priority: "latency"  # "latency" or "throughput"

  # Optimization techniques
  techniques:
    kernel_fusion: true
    operation_reordering: true
    memory_coalescing: true
    shared_memory: true
    tensor_cores: true  # Use Tensor Cores for matrix ops

  # Auto-tuning
  auto_tune:
    enabled: true
    duration_seconds: 300  # Auto-tune for 5 minutes on startup
    save_best_config: true

# Debugging and Development
debug:
  # Debug mode
  enabled: false

  # Validation
  validate_inputs: false
  validate_outputs: false
  check_nan_inf: false

  # Logging
  log_level: "INFO"  # DEBUG, INFO, WARNING, ERROR
  verbose_profiling: false

  # Fallback options
  fallback_to_cpu: false
  disable_optimizations: false
